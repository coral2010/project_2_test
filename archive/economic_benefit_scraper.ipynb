{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import time\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://www.nps.gov/yose/learn/news/newsreleases.htm'\n",
    "browser.visit(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to hold raw scraped data\n",
    "article_links = []\n",
    "headlines = []\n",
    "article_contents = []\n",
    "\n",
    "# empty lists that will hold final scraped data\n",
    "years = []\n",
    "economic_benefits = []\n",
    "job_counts = []\n",
    "visitor_counts = []\n",
    "\n",
    "# empty list to hold final scraped data\n",
    "article_dict = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through pages 1-33 and find links of targeted articles\n",
    "for x in range(1, 34):\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    article_snippets = soup.find_all('li', class_='ListingList-item')\n",
    "    substring = 'Economic Benefit'\n",
    "    \n",
    "    for article_snippet in article_snippets:\n",
    "        snippet_headline = article_snippet.find('h3', class_='ListingResults-title').text\n",
    "        \n",
    "        if substring in snippet_headline:\n",
    "            \n",
    "            end_link = article_snippet.find('a')['href']\n",
    "            article_link = 'https://www.nps.gov' + end_link\n",
    "            article_links.append(article_link)\n",
    "            \n",
    "            \n",
    "    browser.click_link_by_text('Next ')\n",
    "    time.sleep(1)\n",
    "    \n",
    "#article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit each article link and extract content\n",
    "for article_link in article_links:\n",
    "    \n",
    "    browser.visit(article_link)\n",
    "    article_html = browser.html\n",
    "    article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "            \n",
    "    headline = article_soup.find('div', class_='ContentHeader').text\n",
    "    headline = headline.replace('\\n', '')\n",
    "    headlines.append(headline)\n",
    "    \n",
    "    article_content = article_soup.find('div', class_='ArticleTextGroup').text\n",
    "    article_contents.append(article_content)\n",
    "    \n",
    "#headlines\n",
    "#article_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through headlines and extract economic benefit $ amount (in millions)\n",
    "for headline in headlines:\n",
    "    headline_split = headline.split('$')[1]\n",
    "    economic_benefit = headline_split[:3]\n",
    "    economic_benefits.append(economic_benefit)\n",
    "#economic_benefits\n",
    "\n",
    "# loop through article contents and extract year, job count, and visitor count\n",
    "for article_content in article_contents:\n",
    "    year_split = article_content.split('Park in ')[1]\n",
    "    year = year_split[:4]\n",
    "    years.append(year)\n",
    "    \n",
    "    job_split = article_content.split('supported ')[1]\n",
    "    job_count = job_split[:5]\n",
    "    if ',' in job_count:\n",
    "        job_count = job_count.replace(',', '')\n",
    "        job_counts.append(job_count)\n",
    "    elif ' ' in job_count:\n",
    "        job_count = job_count.replace(' ', '')\n",
    "        job_counts.append(job_count)\n",
    "    else: \n",
    "        job_counts.append(job_count)\n",
    "    \n",
    "    visitor_split = article_content.split('shows that')[1]\n",
    "    visitor_count = visitor_split[:10]\n",
    "    visitor_count = visitor_count.replace(',', '').replace('\\xa0', '').replace(' ', '')\n",
    "    visitor_counts.append(visitor_count)\n",
    "\n",
    "#years\n",
    "#job_counts\n",
    "#visitor_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append extract information into a dictionary that will be uploaded into mongodb\n",
    "article_dict.append({'years': years,\n",
    "                    'economic_benefits': economic_benefits,\n",
    "                   'job_counts': job_counts,\n",
    "                   'visitor_counts': visitor_counts})\n",
    "\n",
    "#article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'years': ['2017', '2016', '2015', '2014', '2013', '2012'],\n",
       "  'economic_benefits': ['589', '686', '594', '535', '373', '378'],\n",
       "  'job_counts': ['6666', '7883', '6890', '6261', '5033', '5162'],\n",
       "  'visitor_counts': ['4336889',\n",
       "   '5028868',\n",
       "   '4150217',\n",
       "   '3882642',\n",
       "   '3691192',\n",
       "   '3853404']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append missing 2015 data\n",
    "article_dict[0]['years'].insert(2, '2015')\n",
    "article_dict[0]['economic_benefits'].insert(2, '594')\n",
    "article_dict[0]['job_counts'].insert(2, '6890')\n",
    "article_dict[0]['visitor_counts'].insert(2, '4150217')\n",
    "\n",
    "article_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
